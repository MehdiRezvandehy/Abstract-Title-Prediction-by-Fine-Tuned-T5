# Fine-tuning T5 Model for Title Prediction

The T5 (Text-to-Text Transfer Transformer) model is a transformer-based architecture developed by Google Research. It is designed to handle a wide range of natural language processing (NLP) tasks in a unified manner. T5 is unique in that it frames all NLP tasks as a text-to-text problem, meaning both the input and output are treated as sequences of text.  T5 is a powerful and versatile transformer-based model that approaches NLP tasks in a text-to-text framework, allowing for unified architecture and effective transfer learning. T5's ability to handle diverse tasks with a single model structure makes it a valuable tool in natural language processing. In this notebook, first Off-shelf T5 (pre-trained) models are applied for various tasks including translation, summarization, question/answering .... However, these pre-trained models can be used as baseline prediction, but cannot be directly used in production. Off-shelf results with T5 cannot be used in production; we can only use them as baseline prediction. T5 should be fine-tuned for models in production. In this study, [t5-base](https://huggingface.co/t5-base) model is fine-tuned to predict titles for abstracts.
